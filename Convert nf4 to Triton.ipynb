{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkY6wmyIu8Wp",
        "outputId": "471ec8ed-a69d-4915-9183-ca87f35e8fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 11 13:22:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q triton\n"
      ],
      "metadata": {
        "id": "Wmgi7SGju8_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Triton version:\", triton.__version__)\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdxQBssMu9Cm",
        "outputId": "a45a6814-f495-47d8-a529-3d39857d0b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Triton version: 3.2.0\n",
            "CUDA device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def simulate_nf4_input(batch_size=2, num_tokens=512, dim=4096, dtype=torch.float16):\n",
        "    # Step 1: Create dummy float weights\n",
        "    weights = torch.randn(dim, dim, device=\"cuda\", dtype=dtype)\n",
        "\n",
        "    # Step 2: Normalize and quantize to simulate NF4 (4-bit: 0-15)\n",
        "    absmax = weights.abs().amax(dim=-1, keepdim=True)\n",
        "    normed = weights / (absmax + 1e-5)\n",
        "    quantized = torch.clamp(((normed + 1) * 7.5).round(), 0, 15).to(torch.uint8)\n",
        "\n",
        "    # Step 3: Pack two 4-bit values into 1 byte\n",
        "    low  = quantized[..., ::2]  # even indices\n",
        "    high = quantized[..., 1::2]  # odd indices\n",
        "    packed = (high << 4) | low\n",
        "    packed = packed.contiguous()\n",
        "\n",
        "    # Step 4: Simulate absmax scale per row (like BNB does)\n",
        "    scale = absmax.squeeze(-1)\n",
        "\n",
        "    # Step 5: Validate shapes\n",
        "    print(\"NF4 packed shape:\", packed.shape, \"| dtype:\", packed.dtype)\n",
        "    print(\"Absmax shape:\", scale.shape, \"| dtype:\", scale.dtype)\n",
        "\n",
        "    return packed, scale\n",
        "\n",
        "# Test simulate\n",
        "nf4_weights, absmax = simulate_nf4_input()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYW6aW1nu9FW",
        "outputId": "04cdd23f-9a9c-4f18-968b-5d11f70988f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NF4 packed shape: torch.Size([4096, 2048]) | dtype: torch.uint8\n",
            "Absmax shape: torch.Size([4096]) | dtype: torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 possible NF4 values mapped to floats in [-1, 1]\n",
        "NF4_LUT = torch.tensor([\n",
        "    -1.0, -0.696, -0.478, -0.335, -0.239, -0.168, -0.112, -0.066,\n",
        "     0.066,  0.112,  0.168,  0.239,  0.335,  0.478,  0.696,  1.0\n",
        "], device=\"cuda\", dtype=torch.float16)  # use bfloat16 later if needed\n"
      ],
      "metadata": {
        "id": "UJpeAfwMu9Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def nf4_dequant_kernel(\n",
        "    packed_ptr,       # uint8 [M, N//2]\n",
        "    absmax_ptr,       # float16 [M]\n",
        "    lut_ptr,          # float16 [16]\n",
        "    out_ptr,          # output [M, N] float16 or bfloat16\n",
        "    M, N,\n",
        "    BLOCK_N: tl.constexpr,\n",
        "    DTYPE: tl.constexpr,\n",
        "):\n",
        "    row_idx = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_N)\n",
        "\n",
        "    half_cols = N // 2\n",
        "    # Bounds check: we skip rows out of range\n",
        "    if row_idx >= M:\n",
        "        return\n",
        "\n",
        "    # ---- Load absmax ----\n",
        "    absmax = tl.load(absmax_ptr + row_idx).to(tl.float32)\n",
        "\n",
        "    # ---- Load packed 4-bit weights ----\n",
        "    cols = col_offsets + tl.program_id(1) * BLOCK_N\n",
        "    valid = cols < half_cols\n",
        "\n",
        "    # Each element holds 2 values â†’ [M, N//2]\n",
        "    packed_vals = tl.load(packed_ptr + row_idx * half_cols + cols, mask=valid, other=0)\n",
        "\n",
        "    low_bits = packed_vals & 0x0F  # even\n",
        "    high_bits = (packed_vals >> 4) & 0x0F  # odd\n",
        "\n",
        "    # Lookup float value from LUT\n",
        "    low_fp = tl.load(lut_ptr + low_bits)\n",
        "    high_fp = tl.load(lut_ptr + high_bits)\n",
        "\n",
        "    # Scale by absmax\n",
        "    low_fp = low_fp * absmax\n",
        "    high_fp = high_fp * absmax\n",
        "\n",
        "    # Store into output: each byte becomes two floats\n",
        "    base_out = out_ptr + row_idx * N + 2 * cols\n",
        "    if DTYPE == tl.float16:\n",
        "        low_fp = low_fp.to(tl.float16)\n",
        "        high_fp = high_fp.to(tl.float16)\n",
        "    else:\n",
        "        low_fp = low_fp.to(tl.float16)\n",
        "        high_fp = high_fp.to(tl.float16)\n",
        "\n",
        "    tl.store(base_out + 0, low_fp, mask=valid)\n",
        "    tl.store(base_out + 1, high_fp, mask=valid)\n"
      ],
      "metadata": {
        "id": "Ft2GnIkPu9Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dequant_nf4_triton(packed: torch.Tensor, absmax: torch.Tensor, dtype=torch.float16):\n",
        "    assert packed.dtype == torch.uint8\n",
        "    assert absmax.dtype in [torch.float16, torch.bfloat16]\n",
        "    assert packed.shape[0] == absmax.shape[0]\n",
        "    M, N_half = packed.shape\n",
        "    N = N_half * 2\n",
        "\n",
        "    out = torch.empty((M, N), device=packed.device, dtype=dtype)\n",
        "\n",
        "    # Launch Triton kernel\n",
        "    BLOCK_N = 256  # Can tune this later\n",
        "    grid = (M, (N_half + BLOCK_N - 1) // BLOCK_N)\n",
        "\n",
        "    # Cast LUT dtype if needed\n",
        "    lut = NF4_LUT.to(dtype)\n",
        "\n",
        "    nf4_dequant_kernel[grid](\n",
        "        packed_ptr = packed,\n",
        "        absmax_ptr = absmax,\n",
        "        lut_ptr = lut,\n",
        "        out_ptr = out,\n",
        "        M = M,\n",
        "        N = N,\n",
        "        BLOCK_N = BLOCK_N,\n",
        "        DTYPE = 0 if dtype == torch.float16 else 1,  # Triton uses constexpr switches\n",
        "    )\n",
        "\n",
        "    return out.to(dtype) if dtype == torch.bfloat16 else out\n"
      ],
      "metadata": {
        "id": "BI8YF27Eu9O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dequant_nf4_reference(packed: torch.Tensor, absmax: torch.Tensor, dtype=torch.float16):\n",
        "    M, N_half = packed.shape\n",
        "    N = N_half * 2\n",
        "\n",
        "    out = torch.empty((M, N), device=packed.device, dtype=dtype)\n",
        "    lut = NF4_LUT.to(dtype)\n",
        "\n",
        "    for i in range(M):\n",
        "        row_absmax = absmax[i].item()\n",
        "        row = packed[i]\n",
        "        low = row & 0x0F\n",
        "        high = (row >> 4) & 0x0F\n",
        "\n",
        "        low_fp = lut[low.long()] * row_absmax\n",
        "        high_fp = lut[high.long()] * row_absmax\n",
        "\n",
        "        # Interleave low and high into output\n",
        "        out_row = torch.empty(N, device=packed.device, dtype=dtype)\n",
        "        out_row[0::2] = low_fp\n",
        "        out_row[1::2] = high_fp\n",
        "        out[i] = out_row\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "aoWuWklZu9SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate NF4 packed data\n",
        "M, N = 4096, 2048 * 2  # 4096 x 2048 packed, means original is 4096 x 4096\n",
        "packed = torch.randint(0, 256, (M, N // 2), dtype=torch.uint8, device=\"cuda\")\n",
        "\n",
        "# Simulate corresponding absmax values\n",
        "absmax = torch.rand(M, dtype=torch.float16, device=\"cuda\") * 2.0  # Keep scale reasonable\n"
      ],
      "metadata": {
        "id": "GW2OWhFJypuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_triton = dequant_nf4_triton(packed, absmax, dtype=torch.float16)\n",
        "out_ref = dequant_nf4_reference(packed, absmax, dtype=torch.float16)\n",
        "\n",
        "print(\"Max absolute difference:\", (out_triton - out_ref).abs().max())\n",
        "print(\"Mean absolute difference:\", (out_triton - out_ref).abs().mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYNhX5Dlu9Vb",
        "outputId": "73a3cb1c-5d67-4612-8f7e-dffeb33ae04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max absolute difference: tensor(0., device='cuda:0', dtype=torch.float16)\n",
            "Mean absolute difference: tensor(0., device='cuda:0', dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Define NF4 lookup table (LUT)\n",
        "NF4_LUT = torch.tensor([\n",
        "    -1.0000, -0.6960, -0.4780, -0.3350,\n",
        "    -0.2390, -0.1680, -0.1120, -0.0660,\n",
        "     0.0660,  0.1120,  0.1680,  0.2390,\n",
        "     0.3350,  0.4780,  0.6960,  1.0000,\n",
        "], dtype=torch.float16, device=\"cuda\")\n"
      ],
      "metadata": {
        "id": "VwGb56Mgu9Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(NF4_LUT)\n",
        "assert NF4_LUT.shape == (16,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkWGYbpWGh97",
        "outputId": "a5089f3c-708f-4ec3-9f27-af9b1d9c4de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.0000, -0.6958, -0.4780, -0.3350, -0.2390, -0.1680, -0.1120, -0.0660,\n",
            "         0.0660,  0.1120,  0.1680,  0.2390,  0.3350,  0.4780,  0.6958,  1.0000],\n",
            "       device='cuda:0', dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NF4_LUT = torch.tensor([\n",
        "    -1.0, -0.6958, -0.4780, -0.3350,\n",
        "    -0.2390, -0.1680, -0.1120, -0.0660,\n",
        "     0.0660,  0.1120,  0.1680,  0.2390,\n",
        "     0.3350,  0.4780,  0.6958,  1.0\n",
        "], device=\"cuda\", dtype=torch.float16)\n"
      ],
      "metadata": {
        "id": "tPWlbx6aGiWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def dequant_nf4_kernel(\n",
        "    x_q_ptr, x_scale_ptr, x_out_ptr,\n",
        "    M, N,\n",
        "    lut_ptr,\n",
        "    BLOCK: tl.constexpr\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    if row >= M:\n",
        "        return\n",
        "\n",
        "    offs = tl.arange(0, BLOCK)\n",
        "    mask = offs < N\n",
        "\n",
        "    x_q = tl.load(x_q_ptr + row * N + offs, mask=mask).to(tl.uint8)\n",
        "\n",
        "    # Unpack 2 NF4 values per byte\n",
        "    low  = (x_q & 0x0F).to(tl.int32)\n",
        "    high = ((x_q >> 4) & 0x0F).to(tl.int32)\n",
        "\n",
        "    # LUT dequant\n",
        "    deq_low  = tl.load(lut_ptr + low)\n",
        "    deq_high = tl.load(lut_ptr + high)\n",
        "\n",
        "    # Apply scale\n",
        "    scale = tl.load(x_scale_ptr + row)\n",
        "    deq_low  *= scale\n",
        "    deq_high *= scale\n",
        "\n",
        "    # Compute final output indices\n",
        "    out_ptr = x_out_ptr + row * N * 2\n",
        "    out_offs = offs * 2\n",
        "\n",
        "    # Store interleaved dequantized values\n",
        "    tl.store(out_ptr + out_offs + 0, deq_low, mask=mask)\n",
        "    tl.store(out_ptr + out_offs + 1, deq_high, mask=mask)\n"
      ],
      "metadata": {
        "id": "tgdB0xf5KL_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dequant_nf4_triton(x_q, x_scale, dtype=torch.float16):\n",
        "    M, N = x_q.shape\n",
        "    assert x_q.dtype == torch.uint8\n",
        "    assert x_scale.shape == (M,)\n",
        "    assert x_q.device == x_scale.device\n",
        "\n",
        "    BLOCK = N  # Use full row for BLOCK\n",
        "    x_out = torch.empty((M, N * 2), device=x_q.device, dtype=dtype)\n",
        "\n",
        "    dequant_nf4_kernel[(M,)](\n",
        "        x_q, x_scale, x_out,\n",
        "        M, N,\n",
        "        NF4_LUT.to(dtype=torch.float16),\n",
        "        BLOCK=BLOCK,\n",
        "        num_warps=4,\n",
        "        num_stages=1\n",
        "    )\n",
        "\n",
        "    return x_out\n"
      ],
      "metadata": {
        "id": "04hiB9KFKMCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M, N = 4, 8\n",
        "x_q = torch.randint(0, 256, (M, N), dtype=torch.uint8, device=\"cuda\")\n",
        "x_scale = torch.rand(M, device=\"cuda\", dtype=torch.float32)\n",
        "\n",
        "out_fp16 = dequant_nf4_triton(x_q, x_scale, dtype=torch.float16)\n",
        "print(out_fp16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqD95KyDKMFY",
        "outputId": "b4e51dc8-bb92-47a8-82d6-5ecb2db884b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0411,  0.1196,  0.0113,  0.0822, -0.1196, -0.0289, -0.0576,  0.0822,\n",
            "          0.1196, -0.0576, -0.0411,  0.1719, -0.0113, -0.0576,  0.0411, -0.0192],\n",
            "        [ 0.0077,  0.0232,  0.0077,  0.0331, -0.0116,  0.0232,  0.0692, -0.0165,\n",
            "          0.0077, -0.0165, -0.0232,  0.0481,  0.0046, -0.0331, -0.0046,  0.0165],\n",
            "        [-0.3552, -0.0490, -0.1776, -0.1776, -0.1776,  0.1776, -0.7432, -0.2489,\n",
            "          0.2489, -0.1248,  0.1248,  0.0490, -0.5171,  0.3552, -0.3552,  0.2489],\n",
            "        [-0.0343, -0.0717, -0.0047,  0.0120, -0.0343, -0.0080,  0.0120, -0.0047,\n",
            "          0.0717, -0.0047, -0.0172,  0.0172, -0.0080,  0.0343, -0.0080, -0.0047]],\n",
            "       device='cuda:0', dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U triton\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time\n",
        "\n",
        "# 1. Kernel\n",
        "@triton.jit\n",
        "def dequantize_nf4_kernel(\n",
        "    weight_ptr,     # [rows, cols // 2]\n",
        "    absmax_ptr,     # [rows]\n",
        "    lut_ptr,        # [16]\n",
        "    output_ptr,     # [rows, cols]\n",
        "    rows, cols,\n",
        "    BLOCK_COLS: tl.constexpr,\n",
        "):\n",
        "    row_idx = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_COLS)\n",
        "    row_offset = row_idx * cols\n",
        "    out_ptrs = output_ptr + row_offset + col_offsets\n",
        "\n",
        "    cols_half = cols // 2\n",
        "    mask = col_offsets < cols\n",
        "\n",
        "    qweight_ptrs = weight_ptr + row_idx * cols_half + (col_offsets // 2)\n",
        "    packed_vals = tl.load(qweight_ptrs, mask=(col_offsets // 2) < cols_half, other=0)\n",
        "\n",
        "    is_even = (col_offsets % 2) == 0\n",
        "    nf4_indices = tl.where(is_even, packed_vals & 0x0F, (packed_vals >> 4) & 0x0F)\n",
        "    dequant_vals = tl.load(lut_ptr + nf4_indices.to(tl.int32))\n",
        "\n",
        "    absmax = tl.load(absmax_ptr + row_idx).to(dequant_vals.dtype)\n",
        "    output_vals = dequant_vals * absmax\n",
        "    tl.store(out_ptrs, output_vals, mask=mask)\n",
        "\n",
        "# 2. Python wrapper\n",
        "def get_nf4_lookup_table(dtype=torch.float16):\n",
        "    base = torch.tensor([\n",
        "        -1.0, -0.6962, -0.5257, -0.3949,\n",
        "        -0.2847, -0.1848, -0.0916, 0.0,\n",
        "         0.0916, 0.1848, 0.2847, 0.3949,\n",
        "         0.5257, 0.6962, 1.0, 0.0\n",
        "    ], dtype=torch.float32)\n",
        "    return base.to(dtype)\n",
        "\n",
        "def dequantize_nf4(qweight, absmax, dtype=torch.float16):\n",
        "    rows, cols_half = qweight.shape\n",
        "    cols = cols_half * 2\n",
        "    BLOCK_COLS = 128\n",
        "\n",
        "    output = torch.empty((rows, cols), dtype=dtype, device=qweight.device)\n",
        "    lut = get_nf4_lookup_table(dtype).to(qweight.device)\n",
        "\n",
        "    dequantize_nf4_kernel[(rows,)](\n",
        "        qweight, absmax, lut, output,\n",
        "        rows, cols,\n",
        "        BLOCK_COLS=BLOCK_COLS\n",
        "    )\n",
        "    return output\n",
        "\n",
        "# 3. Test function\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.float16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.float16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "\n",
        "        # Simulate 3 separate quantized weight matrices like Unsloth (up, gate, down)\n",
        "        for _ in range(3):\n",
        "            rows, cols = m, hd\n",
        "            cols_half = cols // 2\n",
        "            qweight = torch.randint(0, 256, (rows, cols_half), dtype=torch.uint8, device=\"cuda\")\n",
        "            absmax = torch.rand((rows,), dtype=dt, device=\"cuda\")\n",
        "\n",
        "            # Warmup\n",
        "            for _ in range(2):\n",
        "                _ = dequantize_fx(qweight, absmax, dtype=dt)\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            # Benchmark\n",
        "            start = time.time()\n",
        "            for _ in range(1000):\n",
        "                _ = dequantize_fx(qweight, absmax, dtype=dt)\n",
        "            torch.cuda.synchronize()\n",
        "            elapsed += time.time() - start\n",
        "\n",
        "    return elapsed\n",
        "\n",
        "# 4. Run\n",
        "print(\"Running benchmark...\")\n",
        "total_time = test_dequantize(dequantize_nf4)\n",
        "print(f\"Total time: {total_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "RNk0zC3UozG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47326d9-b64f-4ce5-e35d-ea990cee5547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running benchmark...\n",
            "Total time: 0.96 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nW0QSZYABKkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IAQ8_V-BKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XmRuBhuBKqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXCfIt2Mu9be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZ5xbhUDu9ev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}